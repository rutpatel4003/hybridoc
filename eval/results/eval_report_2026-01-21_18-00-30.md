# Private-RAG Evaluation Report

**Generated**: 2026-01-21 18:00:30  
**Questions**: 25  
**k**: 4

---

## Summary Metrics

| Metric | Value |
|--------|-------|
| **Hit Rate** | 64.0% |
| **Mean Recall@4** | 66.0% |
| **MRR** | 0.559 |
| **Mean Latency** | 6898 ms |
| **P95 Latency** | 9413 ms |

### By Content Type

| Type | Hit Rate |
|------|----------|
| Tables | 50.0% |
| Text | 76.9% |

---

## Per-Question Results

| ID | Question | Hit | Recall@4 | RR | Latency |
|----|----------|-----|----------|-----|---------|
| txt_1 | What is the main contribution of the Tra... | ✅ | 0.50 | 1.00 | 2912ms |
| txt_2 | What are the three ways attention is use... | ✅ | 1.00 | 0.25 | 13137ms |
| txt_3 | Why did the authors choose self-attentio... | ✅ | 1.00 | 0.50 | 7020ms |
| txt_4 | What is the formula for scaled dot-produ... | ✅ | 1.00 | 1.00 | 8787ms |
| txt_5 | What regularization techniques were used... | ❌ | 0.50 | 0.00 | 4914ms |
| txt_6 | How does positional encoding work in the... | ✅ | 1.00 | 1.00 | 8185ms |
| txt_7 | What optimizer was used to train the Tra... | ❌ | 0.00 | 0.17 | 6471ms |
| txt_8 | What is multi-head attention and why is ... | ✅ | 1.00 | 1.00 | 5251ms |
| txt_9 | What datasets were used for training the... | ❌ | 1.00 | 0.00 | 9413ms |
| txt_10 | How long did it take to train the base T... | ✅ | 1.00 | 1.00 | 8119ms |
| tbl_1 | What BLEU score did the Transformer achi... | ✅ | 1.00 | 1.00 | 6212ms |
| tbl_2 | What is the model dimension (d_model) in... | ✅ | 0.50 | 0.50 | 5832ms |
| tbl_3 | How many attention heads are used in the... | ❌ | 0.00 | 0.20 | 5013ms |
| tbl_4 | What is the feed-forward dimension (d_ff... | ❌ | 0.00 | 0.17 | 7816ms |
| tbl_5 | How does the Transformer compare to prev... | ✅ | 1.00 | 1.00 | 6759ms |
| tbl_6 | What dropout rate was used in the base T... | ❌ | 0.00 | 0.20 | 6866ms |
| tbl_7 | What is the training cost comparison bet... | ✅ | 1.00 | 1.00 | 8634ms |
| tbl_8 | How many encoder and decoder layers are ... | ❌ | 0.00 | 0.00 | 4114ms |
| tbl_9 | What label smoothing value was used duri... | ✅ | 1.00 | 1.00 | 5125ms |
| tbl_10 | What is the difference in parameters bet... | ❌ | 0.50 | 0.20 | 7950ms |
| arch_1 | What are the main components of the Tran... | ✅ | 0.50 | 0.33 | 6149ms |
| arch_2 | How does masking work in the decoder's s... | ✅ | 1.00 | 1.00 | 7202ms |
| ablation_1 | What happens to performance when you var... | ❌ | 0.00 | 0.20 | 8040ms |
| ablation_2 | How does reducing the attention key dime... | ✅ | 1.00 | 1.00 | 5477ms |
| parsing_1 | How well did the Transformer perform on ... | ✅ | 1.00 | 0.25 | 7065ms |

---

## Interpretation

- **Hit Rate**: % of queries where at least one relevant doc was in top-k
- **Recall@k**: Average fraction of expected pages/sources found in top-k  
- **MRR**: Mean Reciprocal Rank — how high the first relevant result ranks
- **Latency**: End-to-end retrieval time (embedding + search + rerank)

## Recommendations

- ⚠️ Hit rate below 70% — consider increasing k or tuning retrieval weights
- ⚠️ Table retrieval underperforming text — check table chunking/BM25 text
- ⚠️ High latency — consider caching or reducing reranker candidates
